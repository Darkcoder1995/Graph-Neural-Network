{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darkcoder1995/Graph-Neural-Network/blob/main/GraphSAGERegressor_ETA_Pred_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmbmLRGasLfc",
        "outputId": "9ea2ad00-e8d3-4783-ad8b-d44b979db8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIDoh4I9sMFD",
        "outputId": "d970ae15-195a-43b8-fdaa-8f7d4cb155df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_scatter, torch_sparse\n",
            "Successfully installed torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M7pcICgxotD",
        "outputId": "7bcbfb6d-b698-433b-88fc-c1318132e214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sodapy\n",
            "  Downloading sodapy-2.2.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from sodapy) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (2025.4.26)\n",
            "Downloading sodapy-2.2.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: sodapy\n",
            "Successfully installed sodapy-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sodapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJlug9_IxqSO",
        "outputId": "edb2642a-862d-41ec-c686-41f74b16f803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openrouteservice\n",
            "  Downloading openrouteservice-2.3.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from openrouteservice) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (2025.4.26)\n",
            "Downloading openrouteservice-2.3.3-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: openrouteservice\n",
            "Successfully installed openrouteservice-2.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openrouteservice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L17mDwgyxsRT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torch_geometric.nn import SAGEConv, GATConv\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from sodapy import Socrata\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import openrouteservice\n",
        "from tqdm import tqdm\n",
        "from openrouteservice import Client\n",
        "from openrouteservice.exceptions import ApiError\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import softmax\n",
        "import torch.optim as optim\n",
        "from torch_scatter import scatter\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld_ZY822xuWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "5bc96e55-b9c5-4288-d5bf-fcf045657015"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'processed_taxi_data (1).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-32388475668c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # # # prompt: read results_df_final csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults_df_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_taxi_data (1).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_taxi_data (1).csv'"
          ]
        }
      ],
      "source": [
        "# # # # prompt: read results_df_final csv file\n",
        "# #\n",
        "results_df_final = pd.read_csv('processed_taxi_data (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLBUi6xjyG2w"
      },
      "outputs": [],
      "source": [
        "# Define weather columns (pickup + dropoff)\n",
        "weather_columns = [\n",
        "    'pickup_PRCP', 'pickup_SNOW', 'pickup_SNWD', 'pickup_WESF', 'pickup_WESD', 'pickup_DAPR',\n",
        "    'pickup_MDPR', 'pickup_ADPT', 'pickup_ASLP', 'pickup_ASTP', 'pickup_AWBT', 'pickup_AWND',\n",
        "    'pickup_RHAV', 'pickup_RHMN', 'pickup_RHMX', 'pickup_TMAX', 'pickup_TMIN', 'pickup_WDF2',\n",
        "    'pickup_WDF5', 'pickup_WSF2', 'pickup_WSF5', 'pickup_WT01', 'pickup_WT08', 'pickup_WT04',\n",
        "    'pickup_WT02', 'pickup_WT06', 'pickup_TAVG', 'pickup_WT09', 'pickup_WT03',\n",
        "    'dropoff_PRCP', 'dropoff_SNOW', 'dropoff_SNWD', 'dropoff_WESF', 'dropoff_WESD', 'dropoff_DAPR',\n",
        "    'dropoff_MDPR', 'dropoff_ADPT', 'dropoff_ASLP', 'dropoff_ASTP', 'dropoff_AWBT', 'dropoff_AWND',\n",
        "    'dropoff_RHAV', 'dropoff_RHMN', 'dropoff_RHMX', 'dropoff_TMAX', 'dropoff_TMIN', 'dropoff_WDF2',\n",
        "    'dropoff_WDF5', 'dropoff_WSF2', 'dropoff_WSF5', 'dropoff_WT01', 'dropoff_WT08', 'dropoff_WT04',\n",
        "    'dropoff_WT02', 'dropoff_WT06', 'dropoff_TAVG', 'dropoff_WT09', 'dropoff_WT03'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4dKf9EPyJLm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Combine trip-related and weather columns\n",
        "scale_columns = [\n",
        "    'trip_distance', 'passenger_count', 'fare_amount', 'tip_amount',\n",
        "    'total_amount', 'ors_distance_m'\n",
        "] + weather_columns  # Include weather data\n",
        "\n",
        "# Ensure numeric types (coerce non-numeric to NaN)\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Remove or replace invalid (negative or NaN) values\n",
        "# Option 1: Fill NaNs with small value before log (safer for log1p)\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].fillna(0)\n",
        "\n",
        "# Optionally clip negatives if log1p is not safe for < -1\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].clip(lower=0)\n",
        "\n",
        "# Step 1: Apply log1p\n",
        "results_df_final_log = results_df_final.copy()\n",
        "results_df_final_log[scale_columns] = np.log1p(results_df_final_log[scale_columns])\n",
        "\n",
        "# Step 2: Scale\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(results_df_final_log[scale_columns])\n",
        "\n",
        "# Step 3: Replace the original values with the scaled ones\n",
        "results_df_final[scale_columns] = scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9KkiXAyLac"
      },
      "outputs": [],
      "source": [
        "# # Ensure datetime columns are parsed correctly\n",
        "# results_df_final['lpep_pickup_datetime'] = pd.to_datetime(results_df_final['lpep_pickup_datetime'])\n",
        "# results_df_final['lpep_dropoff_datetime'] = pd.to_datetime(results_df_final['lpep_dropoff_datetime'])\n",
        "\n",
        "# # Calculate trip duration in minutes\n",
        "# results_df_final['trip_duration'] = (\n",
        "#     results_df_final['lpep_dropoff_datetime'] - results_df_final['lpep_pickup_datetime']\n",
        "# ).dt.total_seconds() / 60\n",
        "\n",
        "# # Extract weather-related columns\n",
        "# weather_columns = [col for col in results_df_final.columns if col.startswith('pickup_') or col.startswith('dropoff_')]\n",
        "\n",
        "# from xgboost import XGBRegressor\n",
        "# import numpy as np\n",
        "\n",
        "# # Define features and target\n",
        "# # Exclude the target variable 'trip_duration' and any identifier columns from features\n",
        "# features_columns = [col for col in results_df_final.columns if col not in ['trip_duration', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']]\n",
        "\n",
        "# X = results_df_final[features_columns]\n",
        "# y = results_df_final['trip_duration']\n",
        "\n",
        "# # Ensure feature and target are in numpy format (XGBoost can also work with pandas DataFrames)\n",
        "# X_np = X.values\n",
        "# y_np = y.values\n",
        "\n",
        "# # Train XGBoost\n",
        "# xgb_model = XGBRegressor()\n",
        "# xgb_model.fit(X_np, y_np)\n",
        "\n",
        "# # Get feature importances\n",
        "# importances = xgb_model.feature_importances_\n",
        "\n",
        "# # Select top features (optional, based on your need)\n",
        "# # You can adjust top_k as needed\n",
        "# top_k = 25\n",
        "# top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "\n",
        "# # Get the names of the top features\n",
        "# top_features_names = [features_columns[i] for i in top_indices]\n",
        "\n",
        "# print(\"Top Features (XGBoost):\", top_features_names)\n",
        "\n",
        "# # If you want to use only the top features for further processing:\n",
        "# X_selected_np = X_np[:, top_indices]\n",
        "\n",
        "# print(f\"Shape of selected features: {X_selected_np.shape}\")\n",
        "\n",
        "# features = []\n",
        "# targets = []\n",
        "\n",
        "# # Feature construction\n",
        "# for i, row in results_df_final.iterrows():\n",
        "#     try:\n",
        "#         base_features = [\n",
        "#             pd.to_numeric(row['trip_distance'], errors='coerce'),\n",
        "#             pd.to_numeric(row['passenger_count'], errors='coerce'),\n",
        "#             pd.to_numeric(row['fare_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['tip_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['total_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_distance_m'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_duration_s'], errors='coerce')\n",
        "#         ]\n",
        "\n",
        "#         weather_features = [pd.to_numeric(row[col], errors='coerce') for col in weather_columns]\n",
        "#         feature_vector = [0 if pd.isna(f) else f for f in base_features + weather_features]\n",
        "\n",
        "#         features.append(feature_vector)\n",
        "#         targets.append(row['trip_duration'])\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Skipping row {i} due to error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvzGAjsKt6Hi"
      },
      "outputs": [],
      "source": [
        "# # Ensure datetime columns are parsed correctly\n",
        "# results_df_final['lpep_pickup_datetime'] = pd.to_datetime(results_df_final['lpep_pickup_datetime'])\n",
        "# results_df_final['lpep_dropoff_datetime'] = pd.to_datetime(results_df_final['lpep_dropoff_datetime'])\n",
        "\n",
        "# # Calculate trip duration in minutes\n",
        "# results_df_final['trip_duration'] = (\n",
        "#     results_df_final['lpep_dropoff_datetime'] - results_df_final['lpep_pickup_datetime']\n",
        "# ).dt.total_seconds() / 60\n",
        "\n",
        "# # Extract weather-related columns (already defined earlier, but redefined here)\n",
        "# # This is safe as it's just defining a list of column names.\n",
        "# # weather_columns = [col for col in results_df_final.columns if col.startswith('pickup_') or col.startswith('dropoff_')]\n",
        "\n",
        "# from xgboost import XGBRegressor\n",
        "# import numpy as np\n",
        "\n",
        "# # Define features and target\n",
        "# # Exclude the target variable 'trip_duration' and any identifier columns from features\n",
        "# features_columns = [col for col in results_df_final.columns if col not in ['trip_duration', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']]\n",
        "\n",
        "# # --- Start of suggested changes ---\n",
        "\n",
        "# # Ensure all feature columns are numeric and handle missing/invalid values\n",
        "# for col in features_columns:\n",
        "#     # Convert column to numeric, coercing errors to NaN\n",
        "#     results_df_final[col] = pd.to_numeric(results_df_final[col], errors='coerce')\n",
        "\n",
        "# # Fill remaining NaNs with a suitable value (e.g., 0 or mean/median)\n",
        "# # Using 0 as in your previous scaling step for consistency, but consider alternatives if appropriate\n",
        "# results_df_final[features_columns] = results_df_final[features_columns].fillna(0)\n",
        "\n",
        "# # Optionally clip negative values if they don't make sense for certain features\n",
        "# results_df_final[features_columns] = results_df_final[features_columns].clip(lower=0)\n",
        "\n",
        "# # --- End of suggested changes ---\n",
        "\n",
        "\n",
        "# X = results_df_final[features_columns]\n",
        "# y = results_df_final['trip_duration']\n",
        "\n",
        "# # Ensure feature and target are in numpy format (XGBoost can also work with pandas DataFrames)\n",
        "# X_np = X.values\n",
        "# y_np = y.values\n",
        "\n",
        "# # Train XGBoost\n",
        "# xgb_model = XGBRegressor()\n",
        "# xgb_model.fit(X_np, y_np)\n",
        "\n",
        "# # Get feature importances\n",
        "# importances = xgb_model.feature_importances_\n",
        "\n",
        "# # Select top features (optional, based on your need)\n",
        "# # You can adjust top_k as needed\n",
        "# top_k = 65\n",
        "# top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "\n",
        "# # Get the names of the top features\n",
        "# top_features_names = [features_columns[i] for i in top_indices]\n",
        "\n",
        "# print(\"Top Features (XGBoost):\", top_features_names)\n",
        "\n",
        "# # If you want to use only the top features for further processing:\n",
        "# X_selected_np = X_np[:, top_indices]\n",
        "\n",
        "# print(f\"Shape of selected features: {X_selected_np.shape}\")\n",
        "\n",
        "# # The following feature construction loop is no longer necessary for preparing data\n",
        "# # for the Autoencoder if you use X_selected_np directly. However, if you intend\n",
        "# # to use the 'features' and 'targets' lists for something else later, you might\n",
        "# # need to adjust this loop based on the cleaned DataFrame. Assuming you want to\n",
        "# # use the cleaned `X_selected_np` and `y_np` for the subsequent Autoencoder step:\n",
        "\n",
        "# # features = [] # No longer needed if using X_selected_np\n",
        "# targets = [] # No longer needed if using y_np\n",
        "\n",
        "# # # Feature construction (commented out as X_selected_np should be used instead)\n",
        "# for i, row in results_df_final.iterrows():\n",
        "#     try:\n",
        "# #         base_features = [\n",
        "# #             pd.to_numeric(row['trip_distance'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['passenger_count'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['fare_amount'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['tip_amount'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['total_amount'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['ors_distance_m'], errors='coerce'),\n",
        "# #             pd.to_numeric(row['ors_duration_s'], errors='coerce')\n",
        "# #         ]\n",
        "\n",
        "# #         weather_features = [pd.to_numeric(row[col], errors='coerce') for col in weather_columns]\n",
        "# #         feature_vector = [0 if pd.isna(f) else f for f in base_features + weather_features]\n",
        "\n",
        "# #         features.append(feature_vector)\n",
        "#         targets.append(row['trip_duration'])\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Skipping row {i} due to error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure datetime columns are parsed correctly\n",
        "results_df_final['lpep_pickup_datetime'] = pd.to_datetime(results_df_final['lpep_pickup_datetime'])\n",
        "results_df_final['lpep_dropoff_datetime'] = pd.to_datetime(results_df_final['lpep_dropoff_datetime'])\n",
        "\n",
        "# Calculate trip duration in minutes\n",
        "results_df_final['trip_duration'] = (\n",
        "    results_df_final['lpep_dropoff_datetime'] - results_df_final['lpep_pickup_datetime']\n",
        ").dt.total_seconds() / 60\n",
        "\n",
        "results_df_final['log_trip_duration'] = np.log1p(results_df_final['trip_duration'])\n",
        "\n",
        "# Extract weather-related columns (already defined earlier, but redefined here)\n",
        "# This is safe as it's just defining a list of column names.\n",
        "# weather_columns = [col for col in results_df_final.columns if col.startswith('pickup_') or col.startswith('dropoff_')]\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "# Exclude the target variable 'trip_duration' and any identifier columns from features\n",
        "features_columns = [col for col in results_df_final.columns if col not in ['log_trip_duration','trip_duration', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']]\n",
        "\n",
        "# --- Start of suggested changes ---\n",
        "\n",
        "# Ensure all feature columns are numeric and handle missing/invalid values\n",
        "for col in features_columns:\n",
        "    # Convert column to numeric, coercing errors to NaN\n",
        "    results_df_final[col] = pd.to_numeric(results_df_final[col], errors='coerce')\n",
        "\n",
        "# Fill remaining NaNs with a suitable value (e.g., 0 or mean/median)\n",
        "# Using 0 as in your previous scaling step for consistency, but consider alternatives if appropriate\n",
        "results_df_final[features_columns] = results_df_final[features_columns].fillna(0)\n",
        "\n",
        "# Optionally clip negative values if they don't make sense for certain features\n",
        "results_df_final[features_columns] = results_df_final[features_columns].clip(lower=0)\n",
        "\n",
        "# --- End of suggested changes ---\n",
        "\n",
        "\n",
        "X = results_df_final[features_columns]\n",
        "y = results_df_final['log_trip_duration']\n",
        "\n",
        "# Ensure feature and target are in numpy format (XGBoost can also work with pandas DataFrames)\n",
        "X_np = X.values\n",
        "y_np = y.values\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.fit(X_np, y_np)\n",
        "\n",
        "# Get feature importances\n",
        "importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of features and their importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': features_columns,\n",
        "    'importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Calculate cumulative importance\n",
        "feature_importance_df['cumulative_importance'] = feature_importance_df['importance'].cumsum()\n",
        "feature_importance_df['cumulative_percentage'] = feature_importance_df['cumulative_importance'] / feature_importance_df['importance'].sum()\n",
        "\n",
        "# Define a cumulative percentage threshold (e.g., 95%)\n",
        "cumulative_threshold = 0.95\n",
        "\n",
        "# Select features that reach the cumulative threshold\n",
        "selected_features_df = feature_importance_df[feature_importance_df['cumulative_percentage'] <= cumulative_threshold]\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features_names = selected_features_df['feature'].tolist()\n",
        "\n",
        "print(f\"Top Features (XGBoost) covering {cumulative_threshold*100}% of total importance:\", selected_features_names)\n",
        "print(f\"Number of selected features: {len(selected_features_names)}\")\n",
        "\n",
        "# If you want to use only the selected features for further processing:\n",
        "X_selected_np = X[selected_features_names].values\n",
        "\n",
        "print(f\"Shape of selected features: {X_selected_np.shape}\")\n",
        "\n",
        "# The following feature construction loop is no longer necessary for preparing data\n",
        "# for the Autoencoder if you use X_selected_np directly. However, if you intend\n",
        "# to use the 'features' and 'targets' lists for something else later, you might\n",
        "# need to adjust this loop based on the cleaned DataFrame. Assuming you want to\n",
        "# use the cleaned `X_selected_np` and `y_np` for the subsequent Autoencoder step:\n",
        "\n",
        "# features = [] # No longer needed if using X_selected_np\n",
        "targets = [] # No longer needed if using y_np\n",
        "\n",
        "# # Feature construction (commented out as X_selected_np should be used instead)\n",
        "for i, row in results_df_final.iterrows():\n",
        "    try:\n",
        "#         base_features = [\n",
        "#             pd.to_numeric(row['trip_distance'], errors='coerce'),\n",
        "#             pd.to_numeric(row['passenger_count'], errors='coerce'),\n",
        "#             pd.to_numeric(row['fare_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['tip_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['total_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_distance_m'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_duration_s'], errors='coerce')\n",
        "#         ]\n",
        "\n",
        "#         weather_features = [pd.to_numeric(row[col], errors='coerce') for col in weather_columns]\n",
        "#         feature_vector = [0 if pd.isna(f) else f for f in base_features + weather_features]\n",
        "\n",
        "#         features.append(feature_vector)\n",
        "        targets.append(row['trip_duration'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping row {i} due to error: {e}\")"
      ],
      "metadata": {
        "id": "rbGpyOERGJPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTJu_-LjyQ4y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define the Autoencoder class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim=32):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, encoding_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK2IBTrFyVfU"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    features_np = np.array(X_selected_np, dtype=np.float32)\n",
        "\n",
        "    input_dim = features_np.shape[1]\n",
        "    encoding_dim = top_k  # Adjust as needed\n",
        "\n",
        "    # Instantiate model and prepare for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ae = Autoencoder(input_dim, encoding_dim).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
        "\n",
        "    # Prepare data loader\n",
        "    X_tensor = torch.tensor(features_np, dtype=torch.float32).to(device)\n",
        "    dataset = TensorDataset(X_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(50):\n",
        "        ae.train()\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            inputs = batch[0]\n",
        "            optimizer.zero_grad()\n",
        "            encoded, decoded = ae(inputs)\n",
        "            loss = criterion(decoded, inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # Extract encoded features\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        features_encoded, _ = ae(X_tensor)\n",
        "\n",
        "    X = features_encoded.cpu()\n",
        "    y = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    print(\"✅ Encoded feature tensor shape (Autoencoder):\", X.shape)\n",
        "    print(\"✅ Target tensor shape:\", y.shape)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Error converting features to numpy array: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5NP7Bmyawp"
      },
      "outputs": [],
      "source": [
        "# # X is already the encoded output from autoencoder\n",
        "# encoded_np = X.numpy()\n",
        "\n",
        "# # Make sure targets is in numpy format\n",
        "# targets_np = np.array(targets)\n",
        "\n",
        "# # Train XGBoost on encoded features\n",
        "# xgb_model = XGBRegressor()\n",
        "# xgb_model.fit(encoded_np, targets_np)\n",
        "\n",
        "# # Select top features\n",
        "# importances = xgb_model.feature_importances_\n",
        "# top_k = 25\n",
        "# top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "# selected_encoded = encoded_np[:, top_indices]\n",
        "\n",
        "# # Convert to tensor for further use\n",
        "# X_tensor_selected = torch.tensor(selected_encoded, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRQyrIZgyeX5"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Ensure X_tensor is on CPU for sklearn\n",
        "X_tensor = X.cpu()\n",
        "X_np = X_tensor.numpy()  # Convert to NumPy for NearestNeighbors\n",
        "\n",
        "# k-NN graph (skip self-loop)\n",
        "k = 10\n",
        "nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X_np)\n",
        "distances, neighbors = nbrs.kneighbors(X_np)\n",
        "\n",
        "edge_index_list = []\n",
        "edge_attr_list = []\n",
        "\n",
        "for i in range(X_np.shape[0]):\n",
        "    for idx, j in enumerate(neighbors[i, 1:]):  # skip self-edge (neighbors[i, 0])\n",
        "        edge_index_list.append([i, j])\n",
        "        edge_attr_list.append(distances[i, idx + 1])  # +1 to skip self-distance\n",
        "\n",
        "# Convert to tensors\n",
        "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
        "\n",
        "print(f\"✅ Edge Index Shape: {edge_index.shape}\")  # [2, num_edges]\n",
        "print(f\"✅ Edge Attr Shape: {edge_attr.shape}\")    # [num_edges]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neCb7xpgykYK"
      },
      "outputs": [],
      "source": [
        "# Ensure `X`, `edge_index`, `edge_attr`, and labels `y` exist\n",
        "y_tensor = torch.tensor(targets, dtype=torch.float)\n",
        "y_tensor = torch.log1p(y_tensor)\n",
        "\n",
        "# Ensure edge_index is correctly formatted\n",
        "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "# Ensure edge attributes exist\n",
        "edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "# ✅ Create PyTorch Geometric Data object (fixes the issue)\n",
        "data = Data(\n",
        "    x=X_tensor,  # Node features\n",
        "    edge_index=edge_index_tensor,  # Connectivity between nodes\n",
        "    edge_attr=edge_attr_tensor,  # Edge attributes (distances)\n",
        "    y=y_tensor  # Target trip duration values\n",
        ")\n",
        "\n",
        "print(f\"✅ Data object created! Node shape: {data.x.shape}, Edge shape: {data.edge_index.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuuAMSEGzGki"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get total node count\n",
        "num_nodes = data.x.shape[0]\n",
        "\n",
        "# Generate indices and split into train/test (80/20)\n",
        "indices = np.arange(num_nodes)\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define boolean masks for PyTorch Geometric\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_indices] = True\n",
        "test_mask[test_indices] = True\n",
        "\n",
        "# Assign masks to `Data` object\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "print(f\"✅ Train/Test masks applied! Train nodes: {train_mask.sum().item()}, Test nodes: {test_mask.sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSWmEdB9HLfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAGEConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, aggr='mean'):\n",
        "        super(SAGEConv, self).__init__()\n",
        "        self.aggr = aggr\n",
        "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Aggregate neighbor messages\n",
        "        if self.aggr == 'mean':\n",
        "            deg = torch.bincount(row, minlength=x.size(0)).clamp(min=1)\n",
        "            aggr_messages = torch.zeros_like(x)\n",
        "            aggr_messages.index_add_(0, row, x[col])\n",
        "            aggr_messages = aggr_messages / deg.unsqueeze(-1)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Aggregation '{self.aggr}' not implemented.\")\n",
        "\n",
        "        # Concatenate with self-node representation and apply linear transformation\n",
        "        out = self.lin(torch.cat([x, aggr_messages], dim=1))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnOPOY4THQJP"
      },
      "outputs": [],
      "source": [
        "class GATConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads=1, concat=True, dropout=0.0, negative_slope=0.2):\n",
        "        super(GATConv, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.dropout = dropout\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "        self.lin = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
        "        self.attn = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin.weight)\n",
        "        nn.init.xavier_uniform_(self.attn)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        x = self.lin(x).view(-1, H, C)  # (N, H, C)\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_i = x[row]  # target node features\n",
        "        x_j = x[col]  # source node features\n",
        "\n",
        "        # Concatenate node pairs and compute attention scores\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)  # (E, H, 2C)\n",
        "        alpha = (alpha * self.attn).sum(dim=-1)  # (E, H)\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = F.softmax(alpha, dim=0)  # Softmax over all edges globally (simplified)\n",
        "\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Weight features by attention scores and aggregate\n",
        "        out = torch.zeros(x.size(0), H, C, device=x.device)\n",
        "        out.index_add_(0, row, x_j * alpha.unsqueeze(-1))  # attention-weighted sum\n",
        "\n",
        "        if self.concat:\n",
        "            out = out.view(-1, H * C)\n",
        "        else:\n",
        "            out = out.mean(dim=1)  # average over heads\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgAwbpSjzJCv"
      },
      "outputs": [],
      "source": [
        "# Define your model\n",
        "class GraphSAGERegressor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, use_attention=False):\n",
        "        super(GraphSAGERegressor, self).__init__()\n",
        "        self.use_attention = use_attention\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        if use_attention:\n",
        "            self.convs.append(GATConv(in_channels, hidden_channels))\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
        "        else:\n",
        "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aoyf1DozN5E"
      },
      "outputs": [],
      "source": [
        "# Prepare device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Send the full graph data to the device\n",
        "data = data.to(device)\n",
        "\n",
        "# Initialize model, optimizer, and loss\n",
        "model = GraphSAGERegressor(\n",
        "    in_channels=data.x.shape[1],\n",
        "    hidden_channels=1024,\n",
        "    out_channels=1,\n",
        "    dropout=0.3,\n",
        "    use_attention=True\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Eval\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(data.x, data.edge_index)\n",
        "        val_loss = loss_fn(val_pred[data.test_mask], data.y[data.test_mask])\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0QwsG6k6kQt"
      },
      "outputs": [],
      "source": [
        "# ✅ MSE: 2472.2166, RMSE: 49.7214, MAE: 5.5530 LR=0.0005\n",
        "# ✅ MSE: 2512.5269, RMSE: 50.1251, MAE: 7.5888 LR=0.0001\n",
        "# ✅ MSE: 2736.7002, RMSE: 52.3135, MAE: 14.5382 LR=0.005\n",
        "# ✅ MSE: 2713.2549, RMSE: 52.0889, MAE: 13.7352 LR=0.001\n",
        "# ✅ MSE: 2753.5601, RMSE: 52.4744, MAE: 15.0962 LR=0.0001\n",
        "# ✅ MSE: 2754.0442, RMSE: 52.4790, MAE: 15.1122 LR=0.0001\n",
        "# ✅ MSE: 2551.4868, RMSE: 50.5122, MAE: 9.2425 LR=0.5\n",
        "# ✅ MSE: 2551.4746, RMSE: 50.5121, MAE: 9.2425 LR=0.75\n",
        "# ✅ MSE: 2551.4775, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# ✅ MSE: 2551.4775, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# INCREASED THE FACTORS FROM 45 TO 65\n",
        "# ✅ MSE: 2754.9719, RMSE: 52.4878, MAE: 15.1427 LR=0.0001\n",
        "# ✅ MSE: 2748.1340, RMSE: 52.4226, MAE: 14.9180 LR=0.001\n",
        "# ✅ MSE: 2551.5813, RMSE: 50.5132, MAE: 9.2480 LR=0.05\n",
        "# ✅ MSE: 2551.4761, RMSE: 50.5121, MAE: 9.2425 LR=0.1\n",
        "#CHANGED LAYERS TO 256 FROM 512\n",
        "# ✅ MSE: 2551.4780, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# ✅ MSE: 2650.4043, RMSE: 51.4821, MAE: 11.5120 LR=0.01\n",
        "# ✅ MSE: 2650.4043, RMSE: 51.4821, MAE: 11.5120 LR=0.01\n",
        "# ✅ MSE: 2551.4658, RMSE: 50.5120, MAE: 9.2425 LR=0.05\n",
        "# ✅ MSE: 2551.4846, RMSE: 50.5122, MAE: 9.2425 LR=0.5\n",
        "#CHANGED LAYERS TO 128 FROM 256\n",
        "# ✅ MSE: 2754.5408, RMSE: 52.4837, MAE: 15.1274 LR=0.0005\n",
        "# ✅ MSE: 2751.6758, RMSE: 52.4564, MAE: 15.0302 LR=0.001\n",
        "#CHANGED LAYERS TO 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-46lN8Gh5xdf",
        "outputId": "2ca02a86-9465-4f01-b4ee-0aee759313a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MSE: 2755.1963, RMSE: 52.4900, MAE: 15.1501\n"
          ]
        }
      ],
      "source": [
        "# Put model in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Predict on the whole data\n",
        "with torch.no_grad():\n",
        "    # Run the model on the full graph\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Filter predictions and targets using the test mask\n",
        "    y_true_log = data.y[data.test_mask].cpu()\n",
        "    y_pred_log = out[data.test_mask].cpu()\n",
        "\n",
        "    # Invert log1p transform\n",
        "    y_true = torch.expm1(y_true_log).numpy()\n",
        "    y_pred = torch.expm1(y_pred_log).numpy()\n",
        "\n",
        "# Compute regression metrics\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f\"✅ MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSmRzqZHzw1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcSmt6MSxhfzV8l920RDRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}