{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darkcoder1995/Graph-Neural-Network/blob/main/GraphSAGERegressor_ETA_Pred_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmbmLRGasLfc",
        "outputId": "868fba1c-4e83-4002-953e-b3d35b4d8a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIDoh4I9sMFD",
        "outputId": "ad80fdc9-c145-4cea-9795-92041c093c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch_scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "Collecting torch_sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_scatter, torch_sparse\n",
            "Successfully installed torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M7pcICgxotD",
        "outputId": "9c7e72a5-cd49-40bd-c571-50ad9b9f3489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sodapy in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from sodapy) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28.1->sodapy) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install sodapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJlug9_IxqSO",
        "outputId": "e1a9d3ae-d2c8-4452-f909-4248cb5150ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openrouteservice in /usr/local/lib/python3.11/dist-packages (2.3.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from openrouteservice) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->openrouteservice) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install openrouteservice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L17mDwgyxsRT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torch_geometric.nn import SAGEConv, GATConv\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from sodapy import Socrata\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import openrouteservice\n",
        "from tqdm import tqdm\n",
        "from openrouteservice import Client\n",
        "from openrouteservice.exceptions import ApiError\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import softmax\n",
        "import torch.optim as optim\n",
        "from torch_scatter import scatter\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld_ZY822xuWU"
      },
      "outputs": [],
      "source": [
        "# # # # prompt: read results_df_final csv file\n",
        "# #\n",
        "results_df_final = pd.read_csv('processed_taxi_data (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLBUi6xjyG2w"
      },
      "outputs": [],
      "source": [
        "# Define weather columns (pickup + dropoff)\n",
        "weather_columns = [\n",
        "    'pickup_PRCP', 'pickup_SNOW', 'pickup_SNWD', 'pickup_WESF', 'pickup_WESD', 'pickup_DAPR',\n",
        "    'pickup_MDPR', 'pickup_ADPT', 'pickup_ASLP', 'pickup_ASTP', 'pickup_AWBT', 'pickup_AWND',\n",
        "    'pickup_RHAV', 'pickup_RHMN', 'pickup_RHMX', 'pickup_TMAX', 'pickup_TMIN', 'pickup_WDF2',\n",
        "    'pickup_WDF5', 'pickup_WSF2', 'pickup_WSF5', 'pickup_WT01', 'pickup_WT08', 'pickup_WT04',\n",
        "    'pickup_WT02', 'pickup_WT06', 'pickup_TAVG', 'pickup_WT09', 'pickup_WT03',\n",
        "    'dropoff_PRCP', 'dropoff_SNOW', 'dropoff_SNWD', 'dropoff_WESF', 'dropoff_WESD', 'dropoff_DAPR',\n",
        "    'dropoff_MDPR', 'dropoff_ADPT', 'dropoff_ASLP', 'dropoff_ASTP', 'dropoff_AWBT', 'dropoff_AWND',\n",
        "    'dropoff_RHAV', 'dropoff_RHMN', 'dropoff_RHMX', 'dropoff_TMAX', 'dropoff_TMIN', 'dropoff_WDF2',\n",
        "    'dropoff_WDF5', 'dropoff_WSF2', 'dropoff_WSF5', 'dropoff_WT01', 'dropoff_WT08', 'dropoff_WT04',\n",
        "    'dropoff_WT02', 'dropoff_WT06', 'dropoff_TAVG', 'dropoff_WT09', 'dropoff_WT03'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4dKf9EPyJLm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Combine trip-related and weather columns\n",
        "scale_columns = [\n",
        "    'trip_distance', 'passenger_count', 'fare_amount', 'tip_amount',\n",
        "    'total_amount', 'ors_distance_m'\n",
        "] + weather_columns  # Include weather data\n",
        "\n",
        "# Ensure numeric types (coerce non-numeric to NaN)\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Remove or replace invalid (negative or NaN) values\n",
        "# Option 1: Fill NaNs with small value before log (safer for log1p)\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].fillna(0)\n",
        "\n",
        "# Optionally clip negatives if log1p is not safe for < -1\n",
        "results_df_final[scale_columns] = results_df_final[scale_columns].clip(lower=0)\n",
        "\n",
        "# Step 1: Apply log1p\n",
        "results_df_final_log = results_df_final.copy()\n",
        "results_df_final_log[scale_columns] = np.log1p(results_df_final_log[scale_columns])\n",
        "\n",
        "# Step 2: Scale\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(results_df_final_log[scale_columns])\n",
        "\n",
        "# Step 3: Replace the original values with the scaled ones\n",
        "results_df_final[scale_columns] = scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9KkiXAyLac"
      },
      "outputs": [],
      "source": [
        "# # Ensure datetime columns are parsed correctly\n",
        "# results_df_final['lpep_pickup_datetime'] = pd.to_datetime(results_df_final['lpep_pickup_datetime'])\n",
        "# results_df_final['lpep_dropoff_datetime'] = pd.to_datetime(results_df_final['lpep_dropoff_datetime'])\n",
        "\n",
        "# # Calculate trip duration in minutes\n",
        "# results_df_final['trip_duration'] = (\n",
        "#     results_df_final['lpep_dropoff_datetime'] - results_df_final['lpep_pickup_datetime']\n",
        "# ).dt.total_seconds() / 60\n",
        "\n",
        "# # Extract weather-related columns\n",
        "# weather_columns = [col for col in results_df_final.columns if col.startswith('pickup_') or col.startswith('dropoff_')]\n",
        "\n",
        "# from xgboost import XGBRegressor\n",
        "# import numpy as np\n",
        "\n",
        "# # Define features and target\n",
        "# # Exclude the target variable 'trip_duration' and any identifier columns from features\n",
        "# features_columns = [col for col in results_df_final.columns if col not in ['trip_duration', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']]\n",
        "\n",
        "# X = results_df_final[features_columns]\n",
        "# y = results_df_final['trip_duration']\n",
        "\n",
        "# # Ensure feature and target are in numpy format (XGBoost can also work with pandas DataFrames)\n",
        "# X_np = X.values\n",
        "# y_np = y.values\n",
        "\n",
        "# # Train XGBoost\n",
        "# xgb_model = XGBRegressor()\n",
        "# xgb_model.fit(X_np, y_np)\n",
        "\n",
        "# # Get feature importances\n",
        "# importances = xgb_model.feature_importances_\n",
        "\n",
        "# # Select top features (optional, based on your need)\n",
        "# # You can adjust top_k as needed\n",
        "# top_k = 25\n",
        "# top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "\n",
        "# # Get the names of the top features\n",
        "# top_features_names = [features_columns[i] for i in top_indices]\n",
        "\n",
        "# print(\"Top Features (XGBoost):\", top_features_names)\n",
        "\n",
        "# # If you want to use only the top features for further processing:\n",
        "# X_selected_np = X_np[:, top_indices]\n",
        "\n",
        "# print(f\"Shape of selected features: {X_selected_np.shape}\")\n",
        "\n",
        "# features = []\n",
        "# targets = []\n",
        "\n",
        "# # Feature construction\n",
        "# for i, row in results_df_final.iterrows():\n",
        "#     try:\n",
        "#         base_features = [\n",
        "#             pd.to_numeric(row['trip_distance'], errors='coerce'),\n",
        "#             pd.to_numeric(row['passenger_count'], errors='coerce'),\n",
        "#             pd.to_numeric(row['fare_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['tip_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['total_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_distance_m'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_duration_s'], errors='coerce')\n",
        "#         ]\n",
        "\n",
        "#         weather_features = [pd.to_numeric(row[col], errors='coerce') for col in weather_columns]\n",
        "#         feature_vector = [0 if pd.isna(f) else f for f in base_features + weather_features]\n",
        "\n",
        "#         features.append(feature_vector)\n",
        "#         targets.append(row['trip_duration'])\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Skipping row {i} due to error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvzGAjsKt6Hi",
        "outputId": "152da69f-e349-421c-a14b-42db3fac0412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Features (XGBoost): ['pickup_SNOW', 'pickup_AWND', 'dropoff_PRCP', 'pickup_PRCP', 'payment_type', 'dropoff_latitude', 'extra', 'pickup_station_lat', 'total_amount', 'ors_duration_s', 'fare_amount', 'pickup_latitude', 'ors_distance_m', 'trip_distance', 'dropoff_SNOW', 'passenger_count', 'tip_amount', 'dropoff_RHAV', 'dropoff_station_lat', 'vendorid', 'ratecodeid', 'pickup_TMIN', 'dropoff_RHMX', 'pickup_WDF5', 'dropoff_WDF5', 'trip_type', 'dropoff_TMIN', 'dropoff_WSF5', 'mta_tax', 'pickup_WSF2', 'dropoff_TMAX', 'improvement_surcharge', 'dropoff_WDF2', 'tolls_amount', 'dropoff_AWND', 'dropoff_WSF2', 'pickup_WSF5', 'pickup_TMAX', 'dropoff_SNWD', 'pickup_WDF2', 'dropoff_ASLP', 'pickup_SNWD', 'dropoff_ASTP', 'dropoff_TAVG', 'pickup_ASLP', 'pickup_RHAV', 'dropoff_RHMN', 'pickup_AWBT', 'pickup_WESF', 'pickup_TAVG', 'dropoff_WT01', 'dropoff_WT03', 'dropoff_WT09', 'dropoff_WT06', 'dropoff_AWBT', 'dropoff_WT08', 'dropoff_WT04', 'dropoff_WT02', 'pickup_MDPR', 'pickup_RHMX', 'dropoff_DAPR', 'dropoff_ADPT', 'dropoff_WESD', 'dropoff_WESF', 'pickup_WT03']\n",
            "Shape of selected features: (10000, 65)\n"
          ]
        }
      ],
      "source": [
        "# Ensure datetime columns are parsed correctly\n",
        "results_df_final['lpep_pickup_datetime'] = pd.to_datetime(results_df_final['lpep_pickup_datetime'])\n",
        "results_df_final['lpep_dropoff_datetime'] = pd.to_datetime(results_df_final['lpep_dropoff_datetime'])\n",
        "\n",
        "# Calculate trip duration in minutes\n",
        "results_df_final['trip_duration'] = (\n",
        "    results_df_final['lpep_dropoff_datetime'] - results_df_final['lpep_pickup_datetime']\n",
        ").dt.total_seconds() / 60\n",
        "\n",
        "# Extract weather-related columns (already defined earlier, but redefined here)\n",
        "# This is safe as it's just defining a list of column names.\n",
        "# weather_columns = [col for col in results_df_final.columns if col.startswith('pickup_') or col.startswith('dropoff_')]\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "# Exclude the target variable 'trip_duration' and any identifier columns from features\n",
        "features_columns = [col for col in results_df_final.columns if col not in ['trip_duration', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']]\n",
        "\n",
        "# --- Start of suggested changes ---\n",
        "\n",
        "# Ensure all feature columns are numeric and handle missing/invalid values\n",
        "for col in features_columns:\n",
        "    # Convert column to numeric, coercing errors to NaN\n",
        "    results_df_final[col] = pd.to_numeric(results_df_final[col], errors='coerce')\n",
        "\n",
        "# Fill remaining NaNs with a suitable value (e.g., 0 or mean/median)\n",
        "# Using 0 as in your previous scaling step for consistency, but consider alternatives if appropriate\n",
        "results_df_final[features_columns] = results_df_final[features_columns].fillna(0)\n",
        "\n",
        "# Optionally clip negative values if they don't make sense for certain features\n",
        "results_df_final[features_columns] = results_df_final[features_columns].clip(lower=0)\n",
        "\n",
        "# --- End of suggested changes ---\n",
        "\n",
        "\n",
        "X = results_df_final[features_columns]\n",
        "y = results_df_final['trip_duration']\n",
        "\n",
        "# Ensure feature and target are in numpy format (XGBoost can also work with pandas DataFrames)\n",
        "X_np = X.values\n",
        "y_np = y.values\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.fit(X_np, y_np)\n",
        "\n",
        "# Get feature importances\n",
        "importances = xgb_model.feature_importances_\n",
        "\n",
        "# Select top features (optional, based on your need)\n",
        "# You can adjust top_k as needed\n",
        "top_k = 65\n",
        "top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "\n",
        "# Get the names of the top features\n",
        "top_features_names = [features_columns[i] for i in top_indices]\n",
        "\n",
        "print(\"Top Features (XGBoost):\", top_features_names)\n",
        "\n",
        "# If you want to use only the top features for further processing:\n",
        "X_selected_np = X_np[:, top_indices]\n",
        "\n",
        "print(f\"Shape of selected features: {X_selected_np.shape}\")\n",
        "\n",
        "# The following feature construction loop is no longer necessary for preparing data\n",
        "# for the Autoencoder if you use X_selected_np directly. However, if you intend\n",
        "# to use the 'features' and 'targets' lists for something else later, you might\n",
        "# need to adjust this loop based on the cleaned DataFrame. Assuming you want to\n",
        "# use the cleaned `X_selected_np` and `y_np` for the subsequent Autoencoder step:\n",
        "\n",
        "# features = [] # No longer needed if using X_selected_np\n",
        "targets = [] # No longer needed if using y_np\n",
        "\n",
        "# # Feature construction (commented out as X_selected_np should be used instead)\n",
        "for i, row in results_df_final.iterrows():\n",
        "    try:\n",
        "#         base_features = [\n",
        "#             pd.to_numeric(row['trip_distance'], errors='coerce'),\n",
        "#             pd.to_numeric(row['passenger_count'], errors='coerce'),\n",
        "#             pd.to_numeric(row['fare_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['tip_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['total_amount'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_distance_m'], errors='coerce'),\n",
        "#             pd.to_numeric(row['ors_duration_s'], errors='coerce')\n",
        "#         ]\n",
        "\n",
        "#         weather_features = [pd.to_numeric(row[col], errors='coerce') for col in weather_columns]\n",
        "#         feature_vector = [0 if pd.isna(f) else f for f in base_features + weather_features]\n",
        "\n",
        "#         features.append(feature_vector)\n",
        "        targets.append(row['trip_duration'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping row {i} due to error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTJu_-LjyQ4y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define the Autoencoder class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim=32):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, encoding_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK2IBTrFyVfU",
        "outputId": "53d0b88b-a67d-44a9-be84-f435c2ef200c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1514.4522\n",
            "Epoch 2, Loss: 47.2993\n",
            "Epoch 3, Loss: 28.8893\n",
            "Epoch 4, Loss: 19.2787\n",
            "Epoch 5, Loss: 15.7135\n",
            "Epoch 6, Loss: 13.3067\n",
            "Epoch 7, Loss: 10.8400\n",
            "Epoch 8, Loss: 8.7416\n",
            "Epoch 9, Loss: 7.3090\n",
            "Epoch 10, Loss: 7.1839\n",
            "Epoch 11, Loss: 6.1528\n",
            "Epoch 12, Loss: 6.8377\n",
            "Epoch 13, Loss: 5.0938\n",
            "Epoch 14, Loss: 4.8743\n",
            "Epoch 15, Loss: 4.8764\n",
            "Epoch 16, Loss: 4.6775\n",
            "Epoch 17, Loss: 3.7048\n",
            "Epoch 18, Loss: 3.4784\n",
            "Epoch 19, Loss: 3.7160\n",
            "Epoch 20, Loss: 3.3902\n",
            "Epoch 21, Loss: 3.5679\n",
            "Epoch 22, Loss: 3.0015\n",
            "Epoch 23, Loss: 3.0245\n",
            "Epoch 24, Loss: 3.3158\n",
            "Epoch 25, Loss: 3.2400\n",
            "Epoch 26, Loss: 2.6027\n",
            "Epoch 27, Loss: 2.7115\n",
            "Epoch 28, Loss: 2.7305\n",
            "Epoch 29, Loss: 3.3720\n",
            "Epoch 30, Loss: 2.8898\n",
            "Epoch 31, Loss: 3.0946\n",
            "Epoch 32, Loss: 2.3285\n",
            "Epoch 33, Loss: 2.1895\n",
            "Epoch 34, Loss: 1.9899\n",
            "Epoch 35, Loss: 1.8493\n",
            "Epoch 36, Loss: 1.7864\n",
            "Epoch 37, Loss: 1.9266\n",
            "Epoch 38, Loss: 2.2651\n",
            "Epoch 39, Loss: 3.0349\n",
            "Epoch 40, Loss: 1.5874\n",
            "Epoch 41, Loss: 2.4347\n",
            "Epoch 42, Loss: 1.4646\n",
            "Epoch 43, Loss: 1.8878\n",
            "Epoch 44, Loss: 1.6280\n",
            "Epoch 45, Loss: 2.3426\n",
            "Epoch 46, Loss: 2.2816\n",
            "Epoch 47, Loss: 1.4618\n",
            "Epoch 48, Loss: 1.6644\n",
            "Epoch 49, Loss: 1.8242\n",
            "Epoch 50, Loss: 1.3372\n",
            "✅ Encoded feature tensor shape (Autoencoder): torch.Size([10000, 65])\n",
            "✅ Target tensor shape: torch.Size([10000])\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    features_np = np.array(X_selected_np, dtype=np.float32)\n",
        "\n",
        "    input_dim = features_np.shape[1]\n",
        "    encoding_dim = top_k  # Adjust as needed\n",
        "\n",
        "    # Instantiate model and prepare for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ae = Autoencoder(input_dim, encoding_dim).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
        "\n",
        "    # Prepare data loader\n",
        "    X_tensor = torch.tensor(features_np, dtype=torch.float32).to(device)\n",
        "    dataset = TensorDataset(X_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(50):\n",
        "        ae.train()\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            inputs = batch[0]\n",
        "            optimizer.zero_grad()\n",
        "            encoded, decoded = ae(inputs)\n",
        "            loss = criterion(decoded, inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # Extract encoded features\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        features_encoded, _ = ae(X_tensor)\n",
        "\n",
        "    X = features_encoded.cpu()\n",
        "    y = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    print(\"✅ Encoded feature tensor shape (Autoencoder):\", X.shape)\n",
        "    print(\"✅ Target tensor shape:\", y.shape)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Error converting features to numpy array: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5NP7Bmyawp"
      },
      "outputs": [],
      "source": [
        "# # X is already the encoded output from autoencoder\n",
        "# encoded_np = X.numpy()\n",
        "\n",
        "# # Make sure targets is in numpy format\n",
        "# targets_np = np.array(targets)\n",
        "\n",
        "# # Train XGBoost on encoded features\n",
        "# xgb_model = XGBRegressor()\n",
        "# xgb_model.fit(encoded_np, targets_np)\n",
        "\n",
        "# # Select top features\n",
        "# importances = xgb_model.feature_importances_\n",
        "# top_k = 25\n",
        "# top_indices = np.argsort(importances)[::-1][:top_k]\n",
        "# selected_encoded = encoded_np[:, top_indices]\n",
        "\n",
        "# # Convert to tensor for further use\n",
        "# X_tensor_selected = torch.tensor(selected_encoded, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRQyrIZgyeX5",
        "outputId": "40641132-fe77-418a-8b7f-b4418be39a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Edge Index Shape: torch.Size([2, 100000])\n",
            "✅ Edge Attr Shape: torch.Size([100000])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Ensure X_tensor is on CPU for sklearn\n",
        "X_tensor = X.cpu()\n",
        "X_np = X_tensor.numpy()  # Convert to NumPy for NearestNeighbors\n",
        "\n",
        "# k-NN graph (skip self-loop)\n",
        "k = 10\n",
        "nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X_np)\n",
        "distances, neighbors = nbrs.kneighbors(X_np)\n",
        "\n",
        "edge_index_list = []\n",
        "edge_attr_list = []\n",
        "\n",
        "for i in range(X_np.shape[0]):\n",
        "    for idx, j in enumerate(neighbors[i, 1:]):  # skip self-edge (neighbors[i, 0])\n",
        "        edge_index_list.append([i, j])\n",
        "        edge_attr_list.append(distances[i, idx + 1])  # +1 to skip self-distance\n",
        "\n",
        "# Convert to tensors\n",
        "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
        "\n",
        "print(f\"✅ Edge Index Shape: {edge_index.shape}\")  # [2, num_edges]\n",
        "print(f\"✅ Edge Attr Shape: {edge_attr.shape}\")    # [num_edges]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neCb7xpgykYK",
        "outputId": "99b794c3-a9b5-4c53-c92d-c67b03e3f00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data object created! Node shape: torch.Size([10000, 65]), Edge shape: torch.Size([2, 100000])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-adfac7b809ef>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
            "<ipython-input-53-adfac7b809ef>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float)\n"
          ]
        }
      ],
      "source": [
        "# Ensure `X`, `edge_index`, `edge_attr`, and labels `y` exist\n",
        "y_tensor = torch.tensor(targets, dtype=torch.float)\n",
        "y_tensor = torch.log1p(y_tensor)\n",
        "\n",
        "# Ensure edge_index is correctly formatted\n",
        "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "# Ensure edge attributes exist\n",
        "edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "# ✅ Create PyTorch Geometric Data object (fixes the issue)\n",
        "data = Data(\n",
        "    x=X_tensor,  # Node features\n",
        "    edge_index=edge_index_tensor,  # Connectivity between nodes\n",
        "    edge_attr=edge_attr_tensor,  # Edge attributes (distances)\n",
        "    y=y_tensor  # Target trip duration values\n",
        ")\n",
        "\n",
        "print(f\"✅ Data object created! Node shape: {data.x.shape}, Edge shape: {data.edge_index.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuuAMSEGzGki",
        "outputId": "406b010a-343a-4880-be20-a57b0193f361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Train/Test masks applied! Train nodes: 8000, Test nodes: 2000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get total node count\n",
        "num_nodes = data.x.shape[0]\n",
        "\n",
        "# Generate indices and split into train/test (80/20)\n",
        "indices = np.arange(num_nodes)\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define boolean masks for PyTorch Geometric\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_indices] = True\n",
        "test_mask[test_indices] = True\n",
        "\n",
        "# Assign masks to `Data` object\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "print(f\"✅ Train/Test masks applied! Train nodes: {train_mask.sum().item()}, Test nodes: {test_mask.sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSWmEdB9HLfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAGEConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, aggr='mean'):\n",
        "        super(SAGEConv, self).__init__()\n",
        "        self.aggr = aggr\n",
        "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Aggregate neighbor messages\n",
        "        if self.aggr == 'mean':\n",
        "            deg = torch.bincount(row, minlength=x.size(0)).clamp(min=1)\n",
        "            aggr_messages = torch.zeros_like(x)\n",
        "            aggr_messages.index_add_(0, row, x[col])\n",
        "            aggr_messages = aggr_messages / deg.unsqueeze(-1)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Aggregation '{self.aggr}' not implemented.\")\n",
        "\n",
        "        # Concatenate with self-node representation and apply linear transformation\n",
        "        out = self.lin(torch.cat([x, aggr_messages], dim=1))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnOPOY4THQJP"
      },
      "outputs": [],
      "source": [
        "class GATConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads=1, concat=True, dropout=0.0, negative_slope=0.2):\n",
        "        super(GATConv, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.dropout = dropout\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "        self.lin = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
        "        self.attn = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin.weight)\n",
        "        nn.init.xavier_uniform_(self.attn)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        x = self.lin(x).view(-1, H, C)  # (N, H, C)\n",
        "        row, col = edge_index\n",
        "\n",
        "        x_i = x[row]  # target node features\n",
        "        x_j = x[col]  # source node features\n",
        "\n",
        "        # Concatenate node pairs and compute attention scores\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)  # (E, H, 2C)\n",
        "        alpha = (alpha * self.attn).sum(dim=-1)  # (E, H)\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = F.softmax(alpha, dim=0)  # Softmax over all edges globally (simplified)\n",
        "\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Weight features by attention scores and aggregate\n",
        "        out = torch.zeros(x.size(0), H, C, device=x.device)\n",
        "        out.index_add_(0, row, x_j * alpha.unsqueeze(-1))  # attention-weighted sum\n",
        "\n",
        "        if self.concat:\n",
        "            out = out.view(-1, H * C)\n",
        "        else:\n",
        "            out = out.mean(dim=1)  # average over heads\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgAwbpSjzJCv"
      },
      "outputs": [],
      "source": [
        "# Define your model\n",
        "class GraphSAGERegressor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5, use_attention=False):\n",
        "        super(GraphSAGERegressor, self).__init__()\n",
        "        self.use_attention = use_attention\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        if use_attention:\n",
        "            self.convs.append(GATConv(in_channels, hidden_channels))\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
        "        else:\n",
        "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aoyf1DozN5E",
        "outputId": "26625a22-969b-4f2f-af90-5b5d018cb2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Train Loss: 6.1594 | Val Loss: 6.2295\n",
            "Epoch 002 | Train Loss: 6.1556 | Val Loss: 6.2247\n",
            "Epoch 003 | Train Loss: 6.1509 | Val Loss: 6.2200\n",
            "Epoch 004 | Train Loss: 6.1461 | Val Loss: 6.2152\n",
            "Epoch 005 | Train Loss: 6.1414 | Val Loss: 6.2104\n",
            "Epoch 006 | Train Loss: 6.1366 | Val Loss: 6.2057\n",
            "Epoch 007 | Train Loss: 6.1319 | Val Loss: 6.2009\n",
            "Epoch 008 | Train Loss: 6.1272 | Val Loss: 6.1961\n",
            "Epoch 009 | Train Loss: 6.1224 | Val Loss: 6.1914\n",
            "Epoch 010 | Train Loss: 6.1177 | Val Loss: 6.1866\n",
            "Epoch 011 | Train Loss: 6.1129 | Val Loss: 6.1819\n",
            "Epoch 012 | Train Loss: 6.1082 | Val Loss: 6.1771\n",
            "Epoch 013 | Train Loss: 6.1035 | Val Loss: 6.1724\n",
            "Epoch 014 | Train Loss: 6.0988 | Val Loss: 6.1677\n",
            "Epoch 015 | Train Loss: 6.0940 | Val Loss: 6.1629\n",
            "Epoch 016 | Train Loss: 6.0893 | Val Loss: 6.1582\n",
            "Epoch 017 | Train Loss: 6.0846 | Val Loss: 6.1534\n",
            "Epoch 018 | Train Loss: 6.0799 | Val Loss: 6.1487\n",
            "Epoch 019 | Train Loss: 6.0752 | Val Loss: 6.1440\n",
            "Epoch 020 | Train Loss: 6.0704 | Val Loss: 6.1392\n",
            "Epoch 021 | Train Loss: 6.0649 | Val Loss: 6.1354\n",
            "Epoch 022 | Train Loss: 6.0615 | Val Loss: 6.1303\n",
            "Epoch 023 | Train Loss: 6.0556 | Val Loss: 6.1242\n",
            "Epoch 024 | Train Loss: 6.0501 | Val Loss: 6.1194\n",
            "Epoch 025 | Train Loss: 6.0457 | Val Loss: 6.1145\n",
            "Epoch 026 | Train Loss: 6.0410 | Val Loss: 6.1094\n",
            "Epoch 027 | Train Loss: 6.0356 | Val Loss: 6.1046\n",
            "Epoch 028 | Train Loss: 6.0309 | Val Loss: 6.1003\n",
            "Epoch 029 | Train Loss: 6.0260 | Val Loss: 6.0964\n",
            "Epoch 030 | Train Loss: 6.0219 | Val Loss: 6.0919\n",
            "Epoch 031 | Train Loss: 6.0169 | Val Loss: 6.0870\n",
            "Epoch 032 | Train Loss: 6.0121 | Val Loss: 6.0818\n",
            "Epoch 033 | Train Loss: 6.0074 | Val Loss: 6.0768\n",
            "Epoch 034 | Train Loss: 6.0027 | Val Loss: 6.0720\n",
            "Epoch 035 | Train Loss: 5.9978 | Val Loss: 6.0673\n",
            "Epoch 036 | Train Loss: 5.9932 | Val Loss: 6.0626\n",
            "Epoch 037 | Train Loss: 5.9888 | Val Loss: 6.0580\n",
            "Epoch 038 | Train Loss: 5.9842 | Val Loss: 6.0533\n",
            "Epoch 039 | Train Loss: 5.9795 | Val Loss: 6.0488\n",
            "Epoch 040 | Train Loss: 5.9748 | Val Loss: 6.0444\n",
            "Epoch 041 | Train Loss: 5.9700 | Val Loss: 6.0401\n",
            "Epoch 042 | Train Loss: 5.9657 | Val Loss: 6.0355\n",
            "Epoch 043 | Train Loss: 5.9608 | Val Loss: 6.0307\n",
            "Epoch 044 | Train Loss: 5.9560 | Val Loss: 6.0258\n",
            "Epoch 045 | Train Loss: 5.9515 | Val Loss: 6.0210\n",
            "Epoch 046 | Train Loss: 5.9466 | Val Loss: 6.0163\n",
            "Epoch 047 | Train Loss: 5.9423 | Val Loss: 6.0117\n",
            "Epoch 048 | Train Loss: 5.9374 | Val Loss: 6.0071\n",
            "Epoch 049 | Train Loss: 5.9326 | Val Loss: 6.0024\n",
            "Epoch 050 | Train Loss: 5.9283 | Val Loss: 5.9978\n",
            "Epoch 051 | Train Loss: 5.9234 | Val Loss: 5.9932\n",
            "Epoch 052 | Train Loss: 5.9193 | Val Loss: 5.9885\n",
            "Epoch 053 | Train Loss: 5.9147 | Val Loss: 5.9840\n",
            "Epoch 054 | Train Loss: 5.9098 | Val Loss: 5.9794\n",
            "Epoch 055 | Train Loss: 5.9052 | Val Loss: 5.9747\n",
            "Epoch 056 | Train Loss: 5.9006 | Val Loss: 5.9700\n",
            "Epoch 057 | Train Loss: 5.8961 | Val Loss: 5.9706\n"
          ]
        }
      ],
      "source": [
        "# Prepare device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Send the full graph data to the device\n",
        "data = data.to(device)\n",
        "\n",
        "# Initialize model, optimizer, and loss\n",
        "model = GraphSAGERegressor(\n",
        "    in_channels=data.x.shape[1],\n",
        "    hidden_channels=1024,\n",
        "    out_channels=1,\n",
        "    dropout=0.3,\n",
        "    use_attention=True\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Eval\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(data.x, data.edge_index)\n",
        "        val_loss = loss_fn(val_pred[data.test_mask], data.y[data.test_mask])\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-46lN8Gh5xdf",
        "outputId": "2ca02a86-9465-4f01-b4ee-0aee759313a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MSE: 2755.1963, RMSE: 52.4900, MAE: 15.1501\n"
          ]
        }
      ],
      "source": [
        "# Put model in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Predict on the whole data\n",
        "with torch.no_grad():\n",
        "    # Run the model on the full graph\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Filter predictions and targets using the test mask\n",
        "    y_true_log = data.y[data.test_mask].cpu()\n",
        "    y_pred_log = out[data.test_mask].cpu()\n",
        "\n",
        "    # Invert log1p transform\n",
        "    y_true = torch.expm1(y_true_log).numpy()\n",
        "    y_pred = torch.expm1(y_pred_log).numpy()\n",
        "\n",
        "# Compute regression metrics\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f\"✅ MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0QwsG6k6kQt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ MSE: 2472.2166, RMSE: 49.7214, MAE: 5.5530 LR=0.0005\n",
        "# ✅ MSE: 2512.5269, RMSE: 50.1251, MAE: 7.5888 LR=0.0001\n",
        "# ✅ MSE: 2736.7002, RMSE: 52.3135, MAE: 14.5382 LR=0.005\n",
        "# ✅ MSE: 2713.2549, RMSE: 52.0889, MAE: 13.7352 LR=0.001\n",
        "# ✅ MSE: 2753.5601, RMSE: 52.4744, MAE: 15.0962 LR=0.0001\n",
        "# ✅ MSE: 2754.0442, RMSE: 52.4790, MAE: 15.1122 LR=0.0001\n",
        "# ✅ MSE: 2551.4868, RMSE: 50.5122, MAE: 9.2425 LR=0.5\n",
        "# ✅ MSE: 2551.4746, RMSE: 50.5121, MAE: 9.2425 LR=0.75\n",
        "# ✅ MSE: 2551.4775, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# ✅ MSE: 2551.4775, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# INCREASED THE FACTORS FROM 45 TO 65\n",
        "# ✅ MSE: 2754.9719, RMSE: 52.4878, MAE: 15.1427 LR=0.0001\n",
        "# ✅ MSE: 2748.1340, RMSE: 52.4226, MAE: 14.9180 LR=0.001\n",
        "# ✅ MSE: 2551.5813, RMSE: 50.5132, MAE: 9.2480 LR=0.05\n",
        "# ✅ MSE: 2551.4761, RMSE: 50.5121, MAE: 9.2425 LR=0.1\n",
        "#CHANGED LAYERS TO 256 FROM 512\n",
        "# ✅ MSE: 2551.4780, RMSE: 50.5122, MAE: 9.2425 LR=0.1\n",
        "# ✅ MSE: 2650.4043, RMSE: 51.4821, MAE: 11.5120 LR=0.01\n",
        "# ✅ MSE: 2650.4043, RMSE: 51.4821, MAE: 11.5120 LR=0.01\n",
        "# ✅ MSE: 2551.4658, RMSE: 50.5120, MAE: 9.2425 LR=0.05\n",
        "# ✅ MSE: 2551.4846, RMSE: 50.5122, MAE: 9.2425 LR=0.5\n",
        "#CHANGED LAYERS TO 128 FROM 256\n",
        "# ✅ MSE: 2754.5408, RMSE: 52.4837, MAE: 15.1274 LR=0.0005\n",
        "# ✅ MSE: 2751.6758, RMSE: 52.4564, MAE: 15.0302 LR=0.001\n",
        "#CHANGED LAYERS TO 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSmRzqZHzw1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhMwD51w+fujsCeppGEvwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}